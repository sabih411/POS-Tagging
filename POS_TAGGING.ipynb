{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "mount_file_id": "1boWh74ssFe4LrJSHnQWHJfqThdLWGgQY",
      "authorship_tag": "ABX9TyOxrrfHX3PSMsiqYvJjyAgC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sabih411/POS-Tagging/blob/main/POS_TAGGING.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Loading Libraries"
      ],
      "metadata": {
        "id": "dePc1TzMQjBT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "from sklearn.feature_extraction import DictVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from gensim.models import FastText\n",
        "import pickle\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "metadata": {
        "id": "ln9a8glKQjyy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Date-Preprocessing"
      ],
      "metadata": {
        "id": "t_qIm4W-0AXZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reading Train Data"
      ],
      "metadata": {
        "id": "KVm2b2wLRfyD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gzip -d /content/train.txt.gz"
      ],
      "metadata": {
        "id": "ldM_pcidHak3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DATA = []\n",
        "word_pos_pair = []\n",
        "with open(\"train.txt\", \"r\") as f_open:\n",
        "    for line in f_open:\n",
        "        line = line.strip()\n",
        "        if line:\n",
        "            s = line.split(\" \")\n",
        "            word = s[0]\n",
        "            token = s[1]\n",
        "            word_pos_pair.append((word, token))\n",
        "        else:\n",
        "            if word_pos_pair:\n",
        "                DATA.append(word_pos_pair)\n",
        "                word_pos_pair= []\n",
        "\n",
        "if word_pos_pair:\n",
        "    DATA.append(word_pos_pair)\n",
        "random.shuffle(DATA)\n",
        "split_index = int(0.8 * len(DATA))\n",
        "Data_train = DATA[:split_index]\n",
        "Data_test = DATA[split_index:]\n",
        "print(\"total data sentences\", len(DATA))\n",
        "\n",
        "###################################\n",
        "#FAST CHECK MODEL DATA (VECT_TRAIN/VECT_TEST)\n",
        "##################################\n",
        "vect_train=[]\n",
        "for i in Data_train:\n",
        "  small=[]\n",
        "  y=[]\n",
        "  for j in i:\n",
        "    small.append(j[0])\n",
        "  vect_train.append(small)\n",
        "vect_test=[]\n",
        "for i in Data_test:\n",
        "  small=[]\n",
        "  y=[]\n",
        "  for j in i:\n",
        "    small.append(j[0])\n",
        "  vect_test.append(small)"
      ],
      "metadata": {
        "id": "bBDy94yOq51-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "595dd0f9-e3d7-4daa-f870-b3256bc091b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total data sentences 8936\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Customized feature Engineering"
      ],
      "metadata": {
        "id": "EsoVOFZURwQX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_feature(token, token_index, sent):\n",
        "    pre_word=['the','The','THE','tHe','ThE','thE','THe','A','AN','a','an','aN']\n",
        "    token_feature = {\n",
        "        'token'             : token,\n",
        "        'is_first'          : token_index == 0,\n",
        "        'is_last'           : token_index == len(sent)-1,\n",
        "\n",
        "        'is_capitalized'    : token[0].upper() == token[0],\n",
        "        'is_all_capitalized': token.upper() == token,\n",
        "        'is_capitals_inside': token[1:].lower() != token[1:],\n",
        "        'is_numeric'        : token.isdigit(),\n",
        "\n",
        "        'prefix-1'          : token[0],\n",
        "        'prefix-2'          : '' if len(token) < 2  else token[:2],\n",
        "        'suffix-1'          : token[-1],\n",
        "        'suffix-2'          : '' if len(token) < 2  else token[-2:],\n",
        "\n",
        "        'prev-token'        : '' if token_index == 0     else sent[token_index - 1][0],\n",
        "        '2-prev-token'      : '' if token_index <= 1     else sent[token_index - 2][0],\n",
        "\n",
        "\n",
        "        'is prev-token the-an-a' : '' if token_index==0  else sent[token_index-1] in pre_word,\n",
        "        'next-token'        : '' if token_index == len(sent) - 1     else sent[token_index + 1][0],\n",
        "        '2-next-token'      : '' if token_index >= len(sent) - 2     else sent[token_index + 2][0]\n",
        "        }\n",
        "    return  token_feature\n",
        "\n",
        "def get_feature_word(token, token_index, sent):\n",
        "    pre_word = {'the', 'a', 'an'}\n",
        "    def word_shape(tk):\n",
        "        return ''.join(['x' if ch.isalpha() and ch.islower()\n",
        "                        else 'X' if ch.isalpha() and ch.isupper()\n",
        "                        else 'd' if ch.isdigit()\n",
        "                        else ch for ch in tk])\n",
        "    def vowel_consonant_count(tk):\n",
        "        vowels = sum(1 for ch in tk if ch.lower() in 'aeiou')\n",
        "        consonants = sum(1 for ch in tk if ch.isalpha() and ch.lower() not in 'aeiou')\n",
        "        return vowels, consonants\n",
        "    v_count, c_count = vowel_consonant_count(token)\n",
        "    token_feature = {\n",
        "\n",
        "        'token_length': len(token),\n",
        "        'word_shape': word_shape(token),\n",
        "        'has_hyphen': '-' in token,\n",
        "        'num_vowels': v_count,\n",
        "        'num_consonants': c_count,\n",
        "    }\n",
        "\n",
        "    return token_feature\n",
        "\n",
        "def form_data(all_sentences):\n",
        "    features   = []\n",
        "    pos_labels = []\n",
        "    for sent in all_sentences:\n",
        "\n",
        "\n",
        "        for token_index, token_pair in enumerate(sent):\n",
        "            token  = token_pair[0]\n",
        "            features.append(get_feature(token, token_index, sent))\n",
        "            pos_label = token_pair[1]\n",
        "            pos_labels.append(pos_label)\n",
        "\n",
        "    return features, pos_labels\n",
        "\n",
        "\n",
        "def form_data_just_words(all_sentences):\n",
        "    features   = []\n",
        "    pos_labels = []\n",
        "    for sent in all_sentences:\n",
        "\n",
        "\n",
        "        for token_index, token_pair in enumerate(sent):\n",
        "            token  = token_pair[0]\n",
        "            features.append(get_feature_word(token, token_index, sent))\n",
        "            pos_label = token_pair[1]\n",
        "            pos_labels.append(pos_label)\n",
        "\n",
        "    return features, pos_labels\n",
        "\n",
        "def form_data_vect(all_sentences):\n",
        "    features   = []\n",
        "    pos_labels = []\n",
        "    model=FastText.load(\"/content/drive/MyDrive/POS_TAGS_WEIGHT/word2vec.model\")\n",
        "    for sent in all_sentences:\n",
        "        for token_index, token_pair in enumerate(sent):\n",
        "            token  = token_pair[0]\n",
        "            features.append(model.wv[token])\n",
        "            pos_label = token_pair[1]\n",
        "            pos_labels.append(pos_label)\n",
        "\n",
        "    return features, pos_labels\n",
        "\n",
        "#UNCOMMENT TO TRAIN GENSIM VECTORIZER\n",
        "# model =  FastText(vect_train, vector_size=50, window=3, min_count=1, sg=1)\n",
        "# model.save(\"/content/drive/MyDrive/POS_TAGS_WEIGHT/word2vec.model\")\n"
      ],
      "metadata": {
        "id": "kXyLz6sQq54j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading Models"
      ],
      "metadata": {
        "id": "mBSW6k66SDO4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##################\n",
        "#MAKING THE DATA FRAME\n",
        "##################\n",
        "big_f=form_data(Data_train)\n",
        "small_f=form_data_just_words(Data_train)\n",
        "vect_f=form_data_vect(Data_train)\n",
        "\n",
        "vectorizer_big_f = DictVectorizer()\n",
        "vectorizer_big_f.fit(big_f[0])\n",
        "vectorized_features_big_f = vectorizer_big_f.transform(big_f[0])\n",
        "\n",
        "vectorizer_small_f = DictVectorizer()\n",
        "vectorizer_small_f.fit(small_f[0])\n",
        "vectorized_features_small_f= vectorizer_small_f.transform(small_f[0])\n",
        "\n",
        "##############################\n",
        "#INITIALIZIING MODELS\n",
        "#############################\n",
        "model_lr= LogisticRegression(max_iter=100)\n",
        "model_svm = SVC(max_iter=300,probability=True)\n",
        "model_nb =  MultinomialNB()\n",
        "\n",
        "model_nb_small= MultinomialNB()\n",
        "\n",
        "model_lr_vect=LogisticRegression(max_iter=100)"
      ],
      "metadata": {
        "id": "s_-sidMXxMwZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Training"
      ],
      "metadata": {
        "id": "vW1BEkZySKWr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##################################\n",
        "#TRAINING\n",
        "##################################\n",
        "model_lr.fit(vectorized_features_big_f, big_f[1])            #model1\n",
        "model_svm.fit(vectorized_features_big_f, big_f[1])           #model2\n",
        "model_nb.fit(vectorized_features_big_f, big_f[1])            #model3\n",
        "\n",
        "model_nb_small.fit(vectorized_features_small_f, small_f[1])  #model4\n",
        "\n",
        "model_lr_vect.fit(vect_f[0], vect_f[1])                      #model5"
      ],
      "metadata": {
        "id": "RE1TTtAh8K2x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Saving Models"
      ],
      "metadata": {
        "id": "njg7RNJtSScy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#####################################################\n",
        "#SAVING MODELS\n",
        "#####################################################\n",
        "filename = 'model_svm.pickle'\n",
        "pickle.dump(model_svm, open(filename, 'wb'))\n",
        "\n",
        "filename = 'model_lr.pickle'\n",
        "pickle.dump(model_lr, open(filename, 'wb'))\n",
        "\n",
        "filename = 'model_nb.pickle'\n",
        "pickle.dump(model_nb, open(filename, 'wb'))\n",
        "\n",
        "filename = 'model_nb_small.pickle'\n",
        "pickle.dump(model_nb_small, open(filename, 'wb'))\n",
        "\n",
        "filename = 'model_lr_vect.pickle'\n",
        "pickle.dump(model_lr_vect, open(filename, 'wb'))\n",
        "################################################################"
      ],
      "metadata": {
        "id": "lHc3RUuYSRFb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Models Evaluation"
      ],
      "metadata": {
        "id": "UpfTej3tSYhc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#####################################\n",
        "# PREDICTIONS\n",
        "####################################\n",
        "\n",
        "big_f_test=form_data(Data_test)\n",
        "small_f_test=form_data_just_words(Data_test)\n",
        "vect_f_test=form_data_vect(Data_test)\n",
        "\n",
        "predicted_labels_model1  = model_lr.predict(vectorizer_big_f.transform(big_f_test[0]))\n",
        "predicted_probs_model1   = model_lr.predict_proba(vectorizer_big_f.transform(big_f_test[0]))\n",
        "acc_score_model1 = accuracy_score(big_f_test[1], predicted_labels_model1)\n",
        "\n",
        "predicted_labels_model2  = model_svm.predict(vectorizer_big_f.transform(big_f_test[0]))\n",
        "predicted_probs_model2   = model_svm.predict_proba(vectorizer_big_f.transform(big_f_test[0]))\n",
        "acc_score_model2 = accuracy_score(big_f_test[1], predicted_labels_model2)\n",
        "\n",
        "predicted_labels_model3  = model_nb.predict(vectorizer_big_f.transform(big_f_test[0]))\n",
        "predicted_probs_model3   = model_nb.predict_proba(vectorizer_big_f.transform(big_f_test[0]))\n",
        "acc_score_model3 = accuracy_score(big_f_test[1],predicted_labels_model3)\n",
        "\n",
        "predicted_labels_model4  = model_nb_small.predict(vectorizer_small_f.transform(small_f_test[0]))\n",
        "predicted_probs_model4   = model_nb_small.predict_proba(vectorizer_small_f.transform(small_f_test[0]))\n",
        "acc_score_model4= accuracy_score(small_f_test[1], predicted_labels_model4)\n",
        "\n",
        "predicted_labels_model5  = model_lr_vect.predict(vect_f_test[0])\n",
        "predicted_probs_model5   = model_lr_vect.predict_proba(vect_f_test[0])\n",
        "acc_score_model5= accuracy_score(vect_f_test[1],predicted_labels_model5)"
      ],
      "metadata": {
        "id": "Kxf1p_Ov0dMz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(acc_score_model1,acc_score_model2,acc_score_model3,acc_score_model4,acc_score_model5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0s5RXfrHtfS4",
        "outputId": "b7b7c3b8-714f-42bd-9b8b-8dcd642d7342"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9663499976484974 0.9321591496966561 0.9118186521187038 0.5312514696891314 0.7792409349574378\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Majority Voting Method"
      ],
      "metadata": {
        "id": "i4KCFm0USeZt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#######################################################\n",
        "#COMBINING ALL MODELS (MAJORITY VOTING)\n",
        "#######################################################\n",
        "final_predictions=[]\n",
        "import numpy as np\n",
        "for predictions in zip(predicted_labels_model1, predicted_labels_model2, predicted_labels_model3, predicted_labels_model4, predicted_labels_model5):\n",
        "\n",
        "    array=np.array(predictions)\n",
        "    b=np.unique(array,return_counts=True)\n",
        "    #print(b[0][np.argmax(b[1])])\n",
        "    final_predictions.append(b[0][np.argmax(b[1])])\n",
        "\n",
        "acc_score_model5= accuracy_score(vect_f_test[1],final_predictions)\n",
        "print(acc_score_model5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jTuLv2DuTv5e",
        "outputId": "bd75f88b-0aa5-48d0-83b9-6dfd0361c25d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9349104077505526\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Best Probabilistic Difference Method"
      ],
      "metadata": {
        "id": "apaoNjISSjRC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#######################################################\n",
        "#COMBINING ALL MODELS (BEST PROBABILISTIC DIFFERENCE)\n",
        "#######################################################\n",
        "\n",
        "model1=[]\n",
        "for j in range(len(predicted_probs_model1)):\n",
        "  model1.append(abs(sorted(predicted_probs_model1[j])[-1] - sorted(predicted_probs_model1[j])[-2]))\n",
        "\n",
        "model2=[]\n",
        "for j in range(len(predicted_probs_model2)):\n",
        "  model2.append(abs(sorted(predicted_probs_model2[j])[-1] - sorted(predicted_probs_model2[j])[-2]))\n",
        "\n",
        "model3=[]\n",
        "for j in range(len(predicted_probs_model3)):\n",
        "  model3.append(abs(sorted(predicted_probs_model3[j])[-1] - sorted(predicted_probs_model3[j])[-2]))\n",
        "\n",
        "model4=[]\n",
        "for j in range(len(predicted_probs_model4)):\n",
        "  model4.append(abs(sorted(predicted_probs_model4[j])[-1] - sorted(predicted_probs_model4[j])[-2]))\n",
        "\n",
        "model5=[]\n",
        "for j in range(len(predicted_probs_model5)):\n",
        "  model5.append(abs(sorted(predicted_probs_model5[j])[-1] - sorted(predicted_probs_model5[j])[-2]))\n",
        "\n",
        "print(len(model1),len(model2),len(model3),len(model4),len(model5))\n",
        "i=0\n",
        "final_predictions=[]\n",
        "for max_prob in zip(model1,model2,model3,model4,model5):\n",
        "    arr=max_prob\n",
        "    index = arr.index(max(arr))\n",
        "\n",
        "    if index==0:\n",
        "      final_predictions.append(predicted_labels_model1[i])\n",
        "    if index==1:\n",
        "      final_predictions.append(predicted_labels_model2[i])\n",
        "    if index==2:\n",
        "      final_predictions.append(predicted_labels_model3[i])\n",
        "    if index==3:\n",
        "      final_predictions.append(predicted_labels_model4[i])\n",
        "    if index==4:\n",
        "      final_predictions.append(predicted_labels_model5[i])\n",
        "    i+=1\n",
        "\n",
        "acc_score_model5= accuracy_score(vect_f_test[1],final_predictions)\n",
        "print(acc_score_model5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rCxUjXaUyiXi",
        "outputId": "d80458af-28a1-4a25-95e4-af1ea3086012"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9459859850444434\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#TESTING NEW DATA"
      ],
      "metadata": {
        "id": "8UlFbE_tUaQt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reading Test Data"
      ],
      "metadata": {
        "id": "9QYd5KIDSw0q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_sentences=[]\n",
        "sentence=[]\n",
        "tag=[]\n",
        "with open(\"/content/unlabeled_test_test.txt\", \"r\") as f_open:\n",
        "    for line in f_open:\n",
        "        line = line.strip()\n",
        "        if line:\n",
        "            s = line\n",
        "            sentence.append(s)\n",
        "        else:\n",
        "            if sentence:\n",
        "                test_sentences.append(sentence)\n",
        "                sentence = []\n",
        "                tag = []\n",
        "if sentence:\n",
        "    test_sentences.append(sentence)"
      ],
      "metadata": {
        "id": "PRT-Nr0wDAFX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_sentences_modified=[]\n",
        "for i in test_sentences:\n",
        "  small=[]\n",
        "  for j in i:\n",
        "    small.append((j,'NNP'))\n",
        "  test_sentences_modified.append(small)"
      ],
      "metadata": {
        "id": "9Ts0fH2WF138"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading saved Models"
      ],
      "metadata": {
        "id": "Z8-S9hS4S42N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#######################################\n",
        "#LOAD MODELS\n",
        "#######################################\n",
        "model_lr= LogisticRegression(max_iter=100)\n",
        "model_svm = SVC(max_iter=300,probability=True)\n",
        "model_nb =  MultinomialNB()\n",
        "\n",
        "model_nb_small= MultinomialNB()\n",
        "\n",
        "model_lr_vect=LogisticRegression(max_iter=100)\n",
        "\n",
        "model_lr = pickle.load(open(\"/content/drive/MyDrive/POS_TAGS_WEIGHT/model_lr.pickle\", \"rb\"))\n",
        "model_svm = pickle.load(open(\"/content/drive/MyDrive/POS_TAGS_WEIGHT/model_svm.pickle\", \"rb\"))\n",
        "model_nb = pickle.load(open(\"/content/drive/MyDrive/POS_TAGS_WEIGHT/model_nb.pickle\", \"rb\"))\n",
        "model_nb_small = pickle.load(open(\"/content/drive/MyDrive/POS_TAGS_WEIGHT/model_nb_small.pickle\", \"rb\"))\n",
        "model_lr_vect = pickle.load(open(\"/content/drive/MyDrive/POS_TAGS_WEIGHT/model_lr_vect.pickle\", \"rb\"))"
      ],
      "metadata": {
        "id": "yXSBIg8CTwEY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Final Evaluation"
      ],
      "metadata": {
        "id": "WiuR7MS_TZ_V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##################################################\n",
        "#Final Evaluation\n",
        "#Please note to run this code it's important to have run the train code before because dict vectorizers are defined under section: Loading Models)\n",
        "##################################################\n",
        "\n",
        "big_f_test=form_data(test_sentences_modified)\n",
        "small_f_test=form_data_just_words(test_sentences_modified)\n",
        "vect_f_test=form_data_vect(test_sentences_modified)\n",
        "\n",
        "predicted_labels_model1  = model_lr.predict(vectorizer_big_f.transform(big_f_test[0]))\n",
        "predicted_probs_model1   = model_lr.predict_proba(vectorizer_big_f.transform(big_f_test[0]))\n",
        "\n",
        "predicted_labels_model2  = model_svm.predict(vectorizer_big_f.transform(big_f_test[0]))\n",
        "predicted_probs_model2   = model_svm.predict_proba(vectorizer_big_f.transform(big_f_test[0]))\n",
        "\n",
        "predicted_labels_model3  = model_nb.predict(vectorizer_big_f.transform(big_f_test[0]))\n",
        "predicted_probs_model3   = model_nb.predict_proba(vectorizer_big_f.transform(big_f_test[0]))\n",
        "\n",
        "predicted_labels_model4  = model_nb_small.predict(vectorizer_small_f.transform(small_f_test[0]))\n",
        "predicted_probs_model4   = model_nb_small.predict_proba(vectorizer_small_f.transform(small_f_test[0]))\n",
        "\n",
        "predicted_labels_model5  = model_lr_vect.predict(vect_f_test[0])\n",
        "predicted_probs_model5   = model_lr_vect.predict_proba(vect_f_test[0])"
      ],
      "metadata": {
        "id": "OqlQpzO-TuLd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#######################################################\n",
        "#COMBINING ALL MODELS (BEST PROBABILISTIC DIFFERENCE)\n",
        "#######################################################\n",
        "\n",
        "model1=[]\n",
        "for j in range(len(predicted_probs_model1)):\n",
        "  model1.append(abs(sorted(predicted_probs_model1[j])[-1] - sorted(predicted_probs_model1[j])[-2]))\n",
        "\n",
        "model2=[]\n",
        "for j in range(len(predicted_probs_model2)):\n",
        "  model2.append(abs(sorted(predicted_probs_model2[j])[-1] - sorted(predicted_probs_model2[j])[-2]))\n",
        "\n",
        "model3=[]\n",
        "for j in range(len(predicted_probs_model3)):\n",
        "  model3.append(abs(sorted(predicted_probs_model3[j])[-1] - sorted(predicted_probs_model3[j])[-2]))\n",
        "\n",
        "model4=[]\n",
        "for j in range(len(predicted_probs_model4)):\n",
        "  model4.append(abs(sorted(predicted_probs_model4[j])[-1] - sorted(predicted_probs_model4[j])[-2]))\n",
        "\n",
        "model5=[]\n",
        "for j in range(len(predicted_probs_model5)):\n",
        "  model5.append(abs(sorted(predicted_probs_model5[j])[-1] - sorted(predicted_probs_model5[j])[-2]))\n",
        "\n",
        "#print(len(model1),len(model2),len(model3),len(model4),len(model5))\n",
        "i=0\n",
        "final_predictions=[]\n",
        "for max_prob in zip(model1,model2,model3,model4,model5):\n",
        "    arr=max_prob\n",
        "    index = arr.index(max(arr))\n",
        "\n",
        "    if index==0:\n",
        "      final_predictions.append(predicted_labels_model1[i])\n",
        "    if index==1:\n",
        "      final_predictions.append(predicted_labels_model2[i])\n",
        "    if index==2:\n",
        "      final_predictions.append(predicted_labels_model3[i])\n",
        "    if index==3:\n",
        "      final_predictions.append(predicted_labels_model4[i])\n",
        "    if index==4:\n",
        "      final_predictions.append(predicted_labels_model5[i])\n",
        "    i+=1\n"
      ],
      "metadata": {
        "id": "s-NU6A_oUY_C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Saving Output File"
      ],
      "metadata": {
        "id": "9zFRfd7BTk1R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "####################################################\n",
        "#MAKING OUTPUT FILE\n",
        "####################################################\n",
        "f=open(\"make.txt\",\"w\")\n",
        "c=0\n",
        "for i in test_sentences:\n",
        "  for j in range(len(i)):\n",
        "     f.write(big_f_test[0][c]['token']+\" \"+final_predictions[c] +\"\\n\")\n",
        "     c+=1\n",
        "  f.write(\"\\n\")\n",
        "f.close()\n"
      ],
      "metadata": {
        "id": "Gb5NLpWXIscW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}